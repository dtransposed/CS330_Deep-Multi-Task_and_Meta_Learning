{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from load_data import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProtoNet(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, num_filters, latent_dim):\n",
    "        super(ProtoNet, self).__init__()\n",
    "        self.num_filters = num_filters\n",
    "        self.latent_dim = latent_dim\n",
    "        num_filter_list = self.num_filters + [latent_dim]\n",
    "        self.convs = []\n",
    "        for i, num_filter in enumerate(num_filter_list):\n",
    "            block_parts = [layers.Conv2D(filters=num_filter, kernel_size=3, padding='SAME', activation='linear')]\n",
    "            block_parts += [layers.BatchNormalization()]\n",
    "            block_parts += [layers.Activation('relu')]\n",
    "            block_parts += [layers.MaxPool2D()]\n",
    "            block = tf.keras.Sequential(block_parts, name = 'conv_block_{}'.format(i))\n",
    "            self.__setattr__(\"conv{}\".format(i), block)\n",
    "            self.convs.append(block)\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "    def call(self, inp):\n",
    "        out = inp\n",
    "        for conv in self.convs:\n",
    "            out = conv(out)\n",
    "        out = self.flatten(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_euclidian_dists(x, y):\n",
    "    \"\"\"\n",
    "    Calculate euclidian distance between two 3D tensors.\n",
    "    Args:\n",
    "        x (tf.Tensor):\n",
    "        y (tf.Tensor):\n",
    "    Returns (tf.Tensor): 2-dim tensor with distances.\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    m = y.shape[0]\n",
    "    x = tf.tile(tf.expand_dims(x, 1), [1, m, 1])\n",
    "    y = tf.tile(tf.expand_dims(y, 0), [n, 1, 1])\n",
    "    return tf.reduce_mean(tf.math.pow(x - y, 2), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProtoLoss(x_latent, q_latent, labels_onehot, num_classes, num_support, num_queries):\n",
    "    \"\"\"\n",
    "        calculates the prototype network loss using the latent representation of x\n",
    "        and the latent representation of the query set\n",
    "        Args:\n",
    "            x_latent: latent representation of supports with shape [N*S, D], where D is the latent dimension\n",
    "            q_latent: latent representation of queries with shape [N*Q, D], where D is the latent dimension\n",
    "            labels_onehot: one-hot encodings of the labels of the queries with shape [N, Q, N]\n",
    "            num_classes: number of classes (N) for classification\n",
    "            num_support: number of examples (S) in the support set\n",
    "            num_queries: number of examples (Q) in the query set\n",
    "        Returns:\n",
    "            ce_loss: the cross entropy loss between the predicted labels and true labels\n",
    "            acc: the accuracy of classification on the queries\n",
    "    \"\"\"\n",
    "    #############################\n",
    "    #### YOUR CODE GOES HERE ####\n",
    "    \n",
    "    # compute the prototypes\n",
    "    x_latent_test = np.reshape(x_latent, (num_classes, -1, x_latent.shape[1]))\n",
    "    prototypes = tf.math.reduce_mean(x_latent_test, axis=1)\n",
    "    dists = calc_euclidian_dists(q_latent, prototypes)\n",
    "    \n",
    "    \n",
    "    # compute cross entropy loss\n",
    "    labels_onehot = np.reshape(labels_onehot, (-1, num_classes))\n",
    "    labels_sparse = tf.argmax(labels_onehot, axis=1)\n",
    "    \n",
    "    ### using cross-entropy loss\n",
    "    softmaxes = tf.nn.softmax(-dists,axis=1)\n",
    "    scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    ce_loss = scce(labels_sparse, softmaxes)\n",
    "    scacc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    acc = scacc(labels_sparse, softmaxes)\n",
    "    \"\"\"\n",
    "    ## using original loss from the original paper \n",
    "    log_p_y = tf.nn.log_softmax(-dists, axis=1)\n",
    "    ce_loss = -tf.reduce_mean(tf.reduce_sum(tf.multiply(labels_onehot, log_p_y), axis=1))\n",
    "    \n",
    "    eq = tf.cast(tf.equal(\n",
    "            tf.cast(tf.argmax(softmaxes, axis=1), tf.int32), \n",
    "            tf.cast(labels_sparse, tf.int32)), tf.float32)\n",
    "    acc = tf.reduce_mean(eq)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # return the cross-entropy loss and accuracy\n",
    "    #ce_loss, acc = None, None\n",
    "    #############################\n",
    "    return ce_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(logs):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    steps = [x for x in range(len(logs['ce_loss']))]\n",
    "    \n",
    "    ax.plot(steps, logs['ce_loss'], label = 'train loss')\n",
    "    ax.plot(steps, logs['val_ce_loss'], label = 'val_loss')\n",
    "    ax.plot(steps, logs['acc'], label = 'train accuracy')\n",
    "    ax.plot(steps, logs['val_acc'], label = 'val accuracy')\n",
    "    ax.set(xlabel='Iterations', title='ProtoNet meta-training')\n",
    "    \n",
    "    \n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "                                                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './omniglot_resized'\n",
    "NO_CLASSES = 3\n",
    "NO_SHOTS = 5\n",
    "NO_QUERIES = 5\n",
    "NO_CLASSES_META_TEST = 3\n",
    "NO_SHOTS_META_TEST = 5\n",
    "NO_QUERIES_META_TEST = 5\n",
    "NO_EPOCHS = 20\n",
    "NO_EPISODES = 100\n",
    "IM_WIDTH, IM_HEIGHT, CHANNELS = 28, 28, 1\n",
    "NUM_FILTERS = 16\n",
    "LATENT_DIM = 16\n",
    "NUM_CONV_LAYERS = 3\n",
    "NO_META_TEST_EPISODES = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer proto_net_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "[Epoch 0/20, Episode 0/100] => meta-training loss: 1.09861, meta-training acc: 0.35417, meta-val loss: 1.09862, meta-val acc: 0.35417\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-bb56384c1f69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         images, labels = data_generator.sample_batch(batch_type = \"meta_train\",\n\u001b[1;32m     17\u001b[0m                                                    \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                                                    shuffle=True)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msupport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mNO_SHOTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/CS330_Deep Multi-Task_and_Meta_Learning/hw2/load_data.py\u001b[0m in \u001b[0;36msample_batch\u001b[0;34m(self, batch_type, batch_size, shuffle, swap)\u001b[0m\n\u001b[1;32m    119\u001b[0m                         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mli\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mli\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_and_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \t\t\timages = [image_file_to_array(\n\u001b[0;32m--> 121\u001b[0;31m \t\t\t\tli[1], self.dim_input) for li in labels_and_images]\n\u001b[0m\u001b[1;32m    122\u001b[0m                         \u001b[0mims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/CS330_Deep Multi-Task_and_Meta_Learning/hw2/load_data.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    119\u001b[0m                         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mli\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mli\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_and_images\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \t\t\timages = [image_file_to_array(\n\u001b[0;32m--> 121\u001b[0;31m \t\t\t\tli[1], self.dim_input) for li in labels_and_images]\n\u001b[0m\u001b[1;32m    122\u001b[0m                         \u001b[0mims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/CS330_Deep Multi-Task_and_Meta_Learning/hw2/load_data.py\u001b[0m in \u001b[0;36mimage_file_to_array\u001b[0;34m(filename, dim_input)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;36m1\u001b[0m \u001b[0mchannel\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \t\"\"\"\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmisc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdim_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.9/envs/ur5-env/lib/python3.6/site-packages/numpy/lib/utils.py\u001b[0m in \u001b[0;36mnewfunc\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;34m\"\"\"`arrayrange` is deprecated, use `arange` instead!\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mnewfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.9/envs/ur5-env/lib/python3.6/site-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(name, flatten, mode)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfromimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.9/envs/ur5-env/lib/python3.6/site-packages/numpy/lib/utils.py\u001b[0m in \u001b[0;36mnewfunc\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;34m\"\"\"`arrayrange` is deprecated, use `arange` instead!\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mnewfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.9/envs/ur5-env/lib/python3.6/site-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mfromimage\u001b[0;34m(im, flatten, mode)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model = ProtoNet([NUM_FILTERS] * NUM_CONV_LAYERS, LATENT_DIM)\n",
    "data_generator = DataGenerator(NO_CLASSES, \n",
    "                               NO_SHOTS + NO_QUERIES,\n",
    "                               NO_CLASSES_META_TEST,\n",
    "                               NO_SHOTS_META_TEST + NO_QUERIES_META_TEST,\n",
    "                               config = {'data_folder': DATA_PATH})\n",
    "logs = {'ce_loss': [],\n",
    "        'acc': [],\n",
    "        'val_ce_loss': [],\n",
    "        'val_acc' : []}\n",
    "\n",
    "for epoch in range(NO_EPOCHS):\n",
    "    for episode in range(NO_EPISODES):\n",
    "        images, labels = data_generator.sample_batch(batch_type = \"meta_train\",\n",
    "                                                   batch_size =  batch_size, \n",
    "                                                   shuffle=True)\n",
    "\n",
    "        support = images[:, :, :NO_SHOTS, :]\n",
    "        query = images[:, :, NO_SHOTS:, :]\n",
    "        labels_ph = labels[:, :, NO_SHOTS:, :]\n",
    "        \n",
    "        num_support = support.shape[2]\n",
    "        num_queries = query.shape[2]\n",
    "        \n",
    "        support = support.reshape(-1, 28, 28, 1)\n",
    "        query = query.reshape(-1, 28, 28, 1)\n",
    "        \n",
    "        #def train_step(inputs, labels, model, loss_object, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            x_latent = model(support)\n",
    "            q_latent = model(query)\n",
    "            ce_loss, acc = ProtoLoss(x_latent, q_latent, labels_ph, NO_CLASSES, num_support, num_queries)\n",
    "        gradients = tape.gradient(ce_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            images, labels = data_generator.sample_batch(batch_type = \"meta_val\",\n",
    "                                                   batch_size =  batch_size, \n",
    "                                                   shuffle=True)\n",
    "            support = images[:, :, :NO_SHOTS, :]\n",
    "            query = images[:, :, NO_SHOTS:, :]\n",
    "            labels_ph = labels[:, :, NO_SHOTS:, :]\n",
    "        \n",
    "            num_support = support.shape[2]\n",
    "            num_queries = query.shape[2]\n",
    "        \n",
    "            support = support.reshape(-1, 28, 28, 1)\n",
    "            query = query.reshape(-1, 28, 28, 1)\n",
    "        \n",
    "            x_latent = model(support)\n",
    "            q_latent = model(query)\n",
    "            val_ce_loss, val_acc = ProtoLoss(x_latent, q_latent, labels_ph, NO_CLASSES, num_support, num_queries)\n",
    "            \n",
    "            logs['ce_loss'].append(ce_loss)\n",
    "            logs['val_ce_loss'].append(val_ce_loss)\n",
    "            logs['acc'].append(acc)\n",
    "            logs['val_acc'].append(val_acc)\n",
    "            \n",
    "            print('[Epoch {}/{}, Episode {}/{}] => meta-training loss: {:.5f}, meta-training acc: {:.5f}, meta-val loss: {:.5f}, meta-val acc: {:.5f}'.format(\n",
    "                                                                                                                                                            epoch,\n",
    "                                                                                                                                                            NO_EPOCHS,\n",
    "                                                                                                                                                            episode,\n",
    "                                                                                                                                                            NO_EPISODES,\n",
    "                                                                                                                                                            ce_loss,\n",
    "                                                                                                                                                            acc,\n",
    "                                                                                                                                                            val_ce_loss,\n",
    "                                                                                                                                                            val_acc))\n",
    "plot_results(logs)\n",
    "model.save_weights('./checkpoints/my_checkpoint')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing...')\n",
    "meta_test_accuracies = []\n",
    "for episode in range(NO_META_TEST_EPISODES):\n",
    "    #############################\n",
    "    #### YOUR CODE GOES HERE ####\n",
    "\n",
    "    # sample a batch of test data and partition into\n",
    "    # support and query sets\n",
    "    images, labels = data_generator.sample_batch(batch_type = \"meta_test\",\n",
    "                                                 batch_size =  batch_size, \n",
    "                                                 shuffle=False)\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    support = images[:, :, :NO_SHOTS_META_TEST, :]\n",
    "    query = images[:, :, NO_SHOTS_META_TEST:, :]\n",
    "    labels_ph = labels[:, :, NO_SHOTS_META_TEST:, :]\n",
    "    \n",
    "    print(support.shape)\n",
    "    print(query.shape)\n",
    "\n",
    "    num_support = support.shape[2]\n",
    "    num_queries = query.shape[2]\n",
    "\n",
    "    support = support.reshape(-1, 28, 28, 1)\n",
    "    query = support.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    print(support.shape)\n",
    "    print(query.shape)\n",
    "    print(labels_ph.shape)\n",
    "    x_latent = model(support)\n",
    "    q_latent = model(query)\n",
    "    test_ce_loss, test_acc = ProtoLoss(x_latent, q_latent, labels_ph, NO_CLASSES_META_TEST, num_support, num_queries)\n",
    "    #############################\n",
    "    meta_test_accuracies.append(ac)\n",
    "    if episode % 50 == 0:\n",
    "        print('[Meta-test episode {}/{}] => loss: {:.5f}, acc: {:.5f}'.format(episode,\n",
    "                                                                              NO_META_TEST_EPISODES, \n",
    "                                                                              test_loss,\n",
    "                                                                              test_acc))\n",
    "avg_acc = np.mean(meta_test_accuracies)\n",
    "stds = np.std(meta_test_accuracies)\n",
    "print('Average Meta-Test Accuracy: {:.5f}, Meta-Test Accuracy Std: {:.5f}'.format(avg_acc, stds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
